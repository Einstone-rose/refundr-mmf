model_config:
  mmbt_custom:
    training_head_type: classification
    losses:
    - type: cross_entropy

    # Either pretraining or classification
    #training_head_type: pretraining
    bert_model_name: bert-base-uncased
    direct_features_input: false
    freeze_text: false
    freeze_modal: false
    freeze_complete_base: false
    finetune_lr_multiplier: 1
    # Dimension of the embedding finally returned by the modal encoder
    modal_hidden_size: 2048
    # Dimension of the embedding finally returned by the text encoder
    text_hidden_size: 768
    # IMPORTANT: Used when classification head is activated
    num_labels: 82947
    modal_encoder:
      type: resnet152
      params:
        pretrained: true
        pool_type: avg
        num_output_features: 1

    use_modal_start_token: true
    use_modal_end_token: true
    text_encoder:
      type: transformer
      params:
        bert_model_name: bert-base-uncased
        # Options below can be overridden to update the bert configuration used
        # to initialize the bert encoder. If some option is missing or
        # if you are using an encoder different then BERT, add extra parameters
        # to your projects configuration file under model_config.mmbt.
        # Those options will automatically override the options for your transformer
        # encoder's configuration. For e.g. vocab_size is missing here, just add
        # vocab_size: x to update the size of the vocabulary with which encoder is
        # initialized. If you update the default values, the transformer you
        # will get will be initialized from scratch.
        hidden_size: 768
        num_hidden_layers: 12
        num_attention_heads: 12
        output_attentions: false
        output_hidden_states: false


dataset_config:
  mmbt_custom:
    return_features_info: false
    processors:
      text_processor:
        type: bert_tokenizer
        params:
          tokenizer_config:
            type: bert-base-uncased
            params:
              do_lower_case: true
          mask_probability: 0
          max_seq_length: 256

scheduler:
  type: warmup_linear
  params:
    num_warmup_steps: 2000
    num_training_steps: ${training.max_updates}

optimizer:
  type: adam_w
  params:
    lr: 1e-5
    eps: 1e-8

evaluation:
  metrics:
  - accuracy
  
training:
  batch_size: 16
  lr_scheduler: true
  max_updates: 300000
  early_stop:
    criteria: nlvr2/accuracy
    minimize: false

checkpoint:
  pretrained_state_mapping:
    bert: bert